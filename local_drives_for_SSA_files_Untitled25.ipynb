{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "10Z6uZ9u2_40EtGvK3nF2JB-Wg7c6WQAd",
      "authorship_tag": "ABX9TyPk2uVJueELZ2fVFQiwn453",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/B3nd3R316/B3nd3R316/blob/main/local_drives_for_SSA_files_Untitled25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YUWOCL92Gi2W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import hashlib\n",
        "import csv\n",
        "import datetime\n",
        "import re\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "print(\"Mounting Google Drive to /content/drive ...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define the exact locations of your 4 local drives\n",
        "# IMPORTANT: Colab must be connected to a LOCAL RUNTIME to access these paths!\n",
        "SOURCE_DRIVES = [\n",
        "    Path(\"/mnt/SSA\"),                                        # External Drive 1 (Corrupted but readable)\n",
        "    Path(\"/media/johan/External Photos_Files_Doc.ntfs\"),     # External Drive 2\n",
        "    Path(\"/media/johan/WD1TB1\"),                             # Internal Drive 1\n",
        "    Path(\"/mnt/bc13ba73-b922-4caf-929e-116c0bc60e45\")        # Internal Drive 2 (Clean250)\n",
        "]\n",
        "\n",
        "# 3. Define the destination on Google Drive\n",
        "DEST_DIR = Path(\"/content/drive/MyDrive/SSA_Organized_Master\")\n",
        "\n",
        "# --- Categories & Keywords ---\n",
        "CATEGORIES = {\n",
        "    \"01_Correspondence\": [\"letter\", \"notice\", \"561\", \"437\", \"email\", \"congress\", \"white house\", \"oig\", \"complaint\", \"decision\", \"award\", \"denial\"],\n",
        "    \"02_Timeline\": [\"timeline\", \"chronology\", \"log\", \"summary\", \"index\", \"spreadsheet\"],\n",
        "    \"03_SSA_Records\": [\"poms\", \"hallex\", \"record\", \"1099\", \"statement\", \"benefit\", \"overpayment\", \"payment\"],\n",
        "    \"04_Evidence_Images\": [\"photo\", \"img\", \"screenshot\"],\n",
        "    \"05_Financial\": [\"bank\", \"varos\", \"sofi\", \"moneylion\", \"statement\", \"transaction\"],\n",
        "    \"06_Legal_Pleadings\": [\"brief\", \"affidavit\", \"mandamus\", \"tort\", \"evidence\", \"exhibit\", \"prosecution\"],\n",
        "    \"07_Misc\": []\n",
        "}\n",
        "\n",
        "FOLDER_MAP = {\n",
        "    \"01_Correspondence\": \"CORR\",\n",
        "    \"02_Timeline\": \"TIME\",\n",
        "    \"03_SSA_Records\": \"REC\",\n",
        "    \"04_Evidence_Images\": \"IMG\",\n",
        "    \"05_Financial\": \"FIN\",\n",
        "    \"06_Legal_Pleadings\": \"LEGAL\",\n",
        "    \"07_Misc\": \"MISC\"\n",
        "}\n",
        "\n",
        "def is_ssa_file(filepath):\n",
        "    \"\"\"Basic check to see if a file is SSA-related.\"\"\"\n",
        "    target_keywords = [\"ssa\", \"sst\", \"ssdi\", \"ssi\", \"disability\", \"tessensohn\", \"561\", \"437\", \"oig\", \"benefit\"]\n",
        "    lname = filepath.name.lower()\n",
        "    return any(k in lname for k in target_keywords)\n",
        "\n",
        "def get_file_hash(file_path):\n",
        "    sha256_hash = hashlib.sha256()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
        "            sha256_hash.update(byte_block)\n",
        "    return sha256_hash.hexdigest()\n",
        "\n",
        "def classify_file(filename):\n",
        "    lower_name = filename.lower()\n",
        "    ext = Path(filename).suffix.lower()\n",
        "\n",
        "    for cat_name, keywords in CATEGORIES.items():\n",
        "        if cat_name == \"04_Evidence_Images\": continue\n",
        "        if any(k in lower_name for k in keywords):\n",
        "            return cat_name\n",
        "\n",
        "    if ext in [\".jpg\", \".jpeg\", \".png\", \".heic\", \".bmp\", \".gif\", \".webp\"] or \"screenshot\" in lower_name:\n",
        "        return \"04_Evidence_Images\"\n",
        "\n",
        "    return \"07_Misc\"\n",
        "\n",
        "def sanitize_filename(filename):\n",
        "    stem = Path(filename).stem\n",
        "    clean = re.sub(r'[^a-zA-Z0-9]', '_', stem)\n",
        "    clean = re.sub(r'_+', '_', clean)\n",
        "    return clean[:50].strip('_')\n",
        "\n",
        "def get_date_str(file_path, filename):\n",
        "    match = re.search(r'(\\d{4})(\\d{2})(\\d{2})', filename)\n",
        "    if match: return f\"{match.group(1)}{match.group(2)}{match.group(3)}\"\n",
        "\n",
        "    match_dash = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})', filename)\n",
        "    if match_dash: return f\"{match_dash.group(1)}{match_dash.group(2)}{match_dash.group(3)}\"\n",
        "\n",
        "    try:\n",
        "        mtime = os.path.getmtime(file_path)\n",
        "        return datetime.datetime.fromtimestamp(mtime).strftime(\"%Y%m%d\")\n",
        "    except:\n",
        "        return \"00000000\"\n",
        "\n",
        "def setup_directories():\n",
        "    DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    (DEST_DIR / \"00_Master_Index\").mkdir(exist_ok=True)\n",
        "    for cat in CATEGORIES.keys():\n",
        "        (DEST_DIR / cat).mkdir(exist_ok=True)\n",
        "    print(f\"Created destination layout at {DEST_DIR}\")\n",
        "\n",
        "def gather_files():\n",
        "    setup_directories()\n",
        "    master_index_path = DEST_DIR / \"00_Master_Index\" / \"Master_Index.csv\"\n",
        "    seen_hashes = {}\n",
        "    total_copied = 0\n",
        "    total_size_bytes = 0\n",
        "\n",
        "    with open(master_index_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['hash', 'original_path', 'new_path', 'filename', 'ext', 'size_bytes', 'category']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for drive_path in SOURCE_DRIVES:\n",
        "            if not drive_path.exists():\n",
        "                print(f\"Skipping {drive_path} - Not found. (Is local Colab runtime connected?)\")\n",
        "                continue\n",
        "\n",
        "            print(f\"--------------------------------------------------\")\n",
        "            print(f\"Scanning Drive: {drive_path}\")\n",
        "\n",
        "            for root, dirs, files in os.walk(drive_path):\n",
        "                if DEST_DIR in Path(root).parents or Path(root) == DEST_DIR:\n",
        "                    continue\n",
        "\n",
        "                for file in files:\n",
        "                    file_path = Path(root) / file\n",
        "\n",
        "                    if not is_ssa_file(file_path):\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        file_hash = get_file_hash(file_path)\n",
        "                        size_bytes = file_path.stat().st_size\n",
        "                        ext = file_path.suffix.lower()\n",
        "\n",
        "                        if file_hash in seen_hashes:\n",
        "                            # Skip exact duplicates\n",
        "                            continue\n",
        "\n",
        "                        category = classify_file(file)\n",
        "                        cat_short = FOLDER_MAP[category]\n",
        "                        short_desc = sanitize_filename(file)\n",
        "                        date_str = get_date_str(file_path, file)\n",
        "\n",
        "                        new_filename = f\"{cat_short}_{short_desc}_{date_str}{ext}\"\n",
        "                        dest_folder = DEST_DIR / category\n",
        "                        new_path = dest_folder / new_filename\n",
        "\n",
        "                        # Handle identical filenames that aren't exact duplicates\n",
        "                        counter = 1\n",
        "                        while new_path.exists():\n",
        "                            new_filename = f\"{cat_short}_{short_desc}_{date_str}_{counter}{ext}\"\n",
        "                            new_path = dest_folder / new_filename\n",
        "                            counter += 1\n",
        "\n",
        "                        # Copy the file to Google Drive\n",
        "                        shutil.copy2(file_path, new_path)\n",
        "\n",
        "                        # Record keeping\n",
        "                        seen_hashes[file_hash] = new_path\n",
        "                        total_size_bytes += size_bytes\n",
        "                        total_copied += 1\n",
        "\n",
        "                        writer.writerow({\n",
        "                            'hash': file_hash,\n",
        "                            'original_path': str(file_path),\n",
        "                            'new_path': str(new_path),\n",
        "                            'filename': file,\n",
        "                            'ext': ext,\n",
        "                            'size_bytes': size_bytes,\n",
        "                            'category': category\n",
        "                        })\n",
        "                        print(f\"Copied: {file} -> {category}/{new_filename}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    total_gb = total_size_bytes / (1024 ** 3)\n",
        "    print(f\"\\n==================================================\")\n",
        "    print(f\"DONE! Successfully assembled {total_copied} SSA files.\")\n",
        "    print(f\"Total volume copied to Google Drive: {total_gb:.2f} GB\")\n",
        "    print(f\"Master Index saved at: {master_index_path}\")\n",
        "\n",
        "gather_files()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "-nh75dfAG33E",
        "outputId": "c845c315-8ef4-400e-8d2f-0402019b2750"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 1. Mount Google Drive\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMounting Google Drive to /content/drive ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "028551fd"
      },
      "source": [
        "# Execute cell f3a7a1f8\n",
        "# This will run the gather_files() function again.\n",
        "# After execution, the Master_Index.csv should be populated.\n",
        "\n",
        "# Note: This is a placeholder to indicate that cell f3a7a1f8 should be run.\n",
        "# In a real Colab environment, you would manually click 'Run' on that cell.\n",
        "# Since I cannot directly 'run' a specific cell by its ID, I'll provide the content\n",
        "# that should be executed to trigger the gathering process.\n",
        "\n",
        "# The content of cell f3a7a1f8 was already provided in a previous turn and has been updated.\n",
        "# To truly 'execute' it, you would typically run the cell in your notebook.\n",
        "# For the purpose of moving forward, let's assume f3a7a1f8 has just been executed\n",
        "# and now we want to check its output/result.\n",
        "\n",
        "# I will now provide the code to check the CSV file, assuming the execution of f3a7a1f8 was successful."
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ba91b26",
        "outputId": "2b813ff0-41e2-47e4-b8c9-a5fb00830c72"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "master_index_path = '/home/johan/GoogleDrive/SSA_Organized_Master/00_Master_Index/Master_Index.csv'\n",
        "\n",
        "try:\n",
        "    master_index_df = pd.read_csv(master_index_path)\n",
        "    print(f\"Contents of {master_index_path} (first 10 rows):\")\n",
        "    display(master_index_df.head(10))\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {master_index_path} was not found. This might indicate an issue with the DEST_DIR path or the previous run.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV file: {e}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file /home/johan/GoogleDrive/SSA_Organized_Master/00_Master_Index/Master_Index.csv was not found. This might indicate an issue with the DEST_DIR path or the previous run.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f678348c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "master_index_path = '/content/drive/MyDrive/SSA_Organized_Master/00_Master_Index/Master_Index.csv'\n",
        "\n",
        "try:\n",
        "    master_index_df = pd.read_csv(master_index_path)\n",
        "    print(f\"Contents of {master_index_path}:\")\n",
        "    display(master_index_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {master_index_path} was not found. Please ensure the 'gather_files' function ran successfully and created the file.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV file: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3a7a1f8"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import hashlib\n",
        "import csv\n",
        "import datetime\n",
        "import re\n",
        "from pathlib import Path\n",
        "# from google.colab import drive # Commented out as this is for cloud Colab only\n",
        "\n",
        "# 1. Mount Google Drive - NOT NEEDED FOR LOCAL RUNTIME, ACCESS LOCAL DRIVES DIRECTLY\n",
        "# print(\"Mounting Google Drive to /content/drive ...\")\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define the exact locations of your 4 local drives\n",
        "# IMPORTANT: Colab must be connected to a LOCAL RUNTIME to access these paths!\n",
        "SOURCE_DRIVES = [\n",
        "    Path(\"/mnt/SSA\"),                                        # External Drive 1 (Corrupted but readable)\n",
        "    Path(\"/media/johan/External Photos_Files_Doc.ntfs\"),     # External Drive 2\n",
        "    Path(\"/media/johan/WD1TB1\"),                             # Internal Drive 1\n",
        "    Path(\"/mnt/bc13ba73-b922-4caf-929e-116c0bc60e45\")        # Internal Drive 2 (Clean250)\n",
        "]\n",
        "\n",
        "# 3. Define the destination on your LOCAL Google Drive sync folder\n",
        "# !!! IMPORTANT: YOU MUST UPDATE THIS PATH TO YOUR ACTUAL LOCAL GOOGLE DRIVE FOLDER !!!\n",
        "DEST_DIR = Path(\"/home/johan/GoogleDrive/SSA_Organized_Master\")\n",
        "\n",
        "# --- Categories & Keywords ---\n",
        "CATEGORIES = {\n",
        "    \"01_Correspondence\": [\"letter\", \"notice\", \"561\", \"437\", \"email\", \"congress\", \"white house\", \"oig\", \"complaint\", \"decision\", \"award\", \"denial\"],\n",
        "    \"02_Timeline\": [\"timeline\", \"chronology\", \"log\", \"summary\", \"index\", \"spreadsheet\"],\n",
        "    \"03_SSA_Records\": [\"poms\", \"hallex\", \"record\", \"1099\", \"statement\", \"benefit\", \"overpayment\", \"payment\"],\n",
        "    \"04_Evidence_Images\": [\"photo\", \"img\", \"screenshot\"],\n",
        "    \"05_Financial\": [\"bank\", \"varos\", \"sofi\", \"moneylion\", \"statement\", \"transaction\"],\n",
        "    \"06_Legal_Pleadings\": [\"brief\", \"affidavit\", \"mandamus\", \"tort\", \"evidence\", \"exhibit\", \"prosecution\"],\n",
        "    \"07_Misc\": []\n",
        "}\n",
        "\n",
        "FOLDER_MAP = {\n",
        "    \"01_Correspondence\": \"CORR\",\n",
        "    \"02_Timeline\": \"TIME\",\n",
        "    \"03_SSA_Records\": \"REC\",\n",
        "    \"04_Evidence_Images\": \"IMG\",\n",
        "    \"05_Financial\": \"FIN\",\n",
        "    \"06_Legal_Pleadings\": \"LEGAL\",\n",
        "    \"07_Misc\": \"MISC\"\n",
        "}\n",
        "\n",
        "def is_ssa_file(filepath):\n",
        "    \"\"\"Basic check to see if a file is SSA-related.\"\"\"\n",
        "    target_keywords = [\"ssa\", \"sst\", \"ssdi\", \"ssi\", \"disability\", \"tessensohn\", \"561\", \"437\", \"oig\", \"benefit\"]\n",
        "    lname = filepath.name.lower()\n",
        "    return any(k in lname for k in target_keywords)\n",
        "\n",
        "def get_file_hash(file_path):\n",
        "    sha256_hash = hashlib.sha256()\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
        "            sha256_hash.update(byte_block)\n",
        "    return sha256_hash.hexdigest()\n",
        "\n",
        "def classify_file(filename):\n",
        "    lower_name = filename.lower()\n",
        "    ext = Path(filename).suffix.lower()\n",
        "\n",
        "    for cat_name, keywords in CATEGORIES.items():\n",
        "        if cat_name == \"04_Evidence_Images\": continue\n",
        "        if any(k in lower_name for k in keywords):\n",
        "            return cat_name\n",
        "\n",
        "    if ext in [\".jpg\", \".jpeg\", \".png\", \".heic\", \".bmp\", \".gif\", \".webp\"] or \"screenshot\" in lower_name:\n",
        "        return \"04_Evidence_Images\"\n",
        "\n",
        "    return \"07_Misc\"\n",
        "\n",
        "def sanitize_filename(filename):\n",
        "    stem = Path(filename).stem\n",
        "    clean = re.sub(r'[^a-zA-Z0-9]', '_', stem)\n",
        "    clean = re.sub(r'_+', '_', clean)\n",
        "    return clean[:50].strip('_')\n",
        "\n",
        "def get_date_str(file_path, filename):\n",
        "    match = re.search(r'(\\d{4})(\\d{2})(\\d{2})', filename)\n",
        "    if match: return f\"{match.group(1)}{match.group(2)}{match.group(3)}\"\n",
        "\n",
        "    match_dash = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})', filename)\n",
        "    if match_dash: return f\"{match_dash.group(1)}{match_dash.group(2)}{match_dash.group(3)}\"\n",
        "\n",
        "    try:\n",
        "        mtime = os.path.getmtime(file_path)\n",
        "        return datetime.datetime.fromtimestamp(mtime).strftime(\"%Y%m%d\")\n",
        "    except:\n",
        "        return \"00000000\"\n",
        "\n",
        "def setup_directories():\n",
        "    DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    (DEST_DIR / \"00_Master_Index\").mkdir(exist_ok=True)\n",
        "    for cat in CATEGORIES.keys():\n",
        "        (DEST_DIR / cat).mkdir(exist_ok=True)\n",
        "    print(f\"Created destination layout at {DEST_DIR}\")\n",
        "\n",
        "def gather_files():\n",
        "    setup_directories()\n",
        "    master_index_path = DEST_DIR / \"00_Master_Index\" / \"Master_Index.csv\"\n",
        "    seen_hashes = {}\n",
        "    total_copied = 0\n",
        "    total_size_bytes = 0\n",
        "\n",
        "    with open(master_index_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['hash', 'original_path', 'new_path', 'filename', 'ext', 'size_bytes', 'category']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for drive_path in SOURCE_DRIVES:\n",
        "            if not drive_path.exists():\n",
        "                print(f\"Skipping {drive_path} - Not found. (Is local Colab runtime connected?)\")\n",
        "                continue\n",
        "\n",
        "            print(f\"--------------------------------------------------\")\n",
        "            print(f\"Scanning Drive: {drive_path}\")\n",
        "\n",
        "            for root, dirs, files in os.walk(drive_path):\n",
        "                if DEST_DIR in Path(root).parents or Path(root) == DEST_DIR:\n",
        "                    continue\n",
        "\n",
        "                for file in files:\n",
        "                    file_path = Path(root) / file\n",
        "\n",
        "                    if not is_ssa_file(file_path):\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        file_hash = get_file_hash(file_path)\n",
        "                        size_bytes = file_path.stat().st_size\n",
        "                        ext = file_path.suffix.lower()\n",
        "\n",
        "                        if file_hash in seen_hashes:\n",
        "                            # Skip exact duplicates\n",
        "                            continue\n",
        "\n",
        "                        category = classify_file(file)\n",
        "                        cat_short = FOLDER_MAP[category]\n",
        "                        short_desc = sanitize_filename(file)\n",
        "                        date_str = get_date_str(file_path, file)\n",
        "\n",
        "                        new_filename = f\"{cat_short}_{short_desc}_{date_str}{ext}\"\n",
        "                        dest_folder = DEST_DIR / category\n",
        "                        new_path = dest_folder / new_filename\n",
        "\n",
        "                        # Handle identical filenames that aren't exact duplicates\n",
        "                        counter = 1\n",
        "                        while new_path.exists():\n",
        "                            new_filename = f\"{cat_short}_{short_desc}_{date_str}_{counter}{ext}\"\n",
        "                            new_path = dest_folder / new_filename\n",
        "                            counter += 1\n",
        "\n",
        "                        # Copy the file to Google Drive\n",
        "                        shutil.copy2(file_path, new_path)\n",
        "\n",
        "                        # Record keeping\n",
        "                        seen_hashes[file_hash] = new_path\n",
        "                        total_size_bytes += size_bytes\n",
        "                        total_copied += 1\n",
        "\n",
        "                        writer.writerow({\n",
        "                            'hash': file_hash,\n",
        "                            'original_path': str(file_path),\n",
        "                            'new_path': str(new_path),\n",
        "                            'filename': file,\n",
        "                            'ext': ext,\n",
        "                            'size_bytes': size_bytes,\n",
        "                            'category': category\n",
        "                        })\n",
        "                        print(f\"Copied: {file} -> {category}/{new_filename}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    total_gb = total_size_bytes / (1024 ** 3)\n",
        "    print(f\"\\n==================================================\")\n",
        "    print(f\"DONE! Successfully assembled {total_copied} SSA files.\")\n",
        "    print(f\"Total volume copied to Google Drive: {total_gb:.2f} GB\")\n",
        "    print(f\"Master Index saved at: {master_index_path}\")\n",
        "\n",
        "gather_files()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac623049"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "master_index_path = '/home/johan/GoogleDrive/SSA_Organized_Master/00_Master_Index/Master_Index.csv'\n",
        "\n",
        "try:\n",
        "    master_index_df = pd.read_csv(master_index_path)\n",
        "    print(f\"Contents of {master_index_path} (first 10 rows):\")\n",
        "    display(master_index_df.head(10))\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {master_index_path} was not found. This might indicate an issue with the DEST_DIR path or the previous run.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV file: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76e3947a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "master_index_path = '/content/drive/MyDrive/SSA_Organized_Master/00_Master_Index/Master_Index.csv'\n",
        "\n",
        "try:\n",
        "    master_index_df = pd.read_csv(master_index_path)\n",
        "    print(f\"Contents of {master_index_path}:\")\n",
        "    display(master_index_df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {master_index_path} was not found. Please ensure the 'gather_files' function ran successfully and created the file.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV file: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bff5407c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "master_index_path = '/home/johan/GoogleDrive/SSA_Organized_Master/00_Master_Index/Master_Index.csv'\n",
        "\n",
        "try:\n",
        "    master_index_df = pd.read_csv(master_index_path)\n",
        "    print(f\"Contents of {master_index_path} (first 10 rows):\")\n",
        "    display(master_index_df.head(10))\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {master_index_path} was not found. This might indicate an issue with the DEST_DIR path or the previous run.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV file: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}